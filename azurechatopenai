from langchain.chat_models import AzureChatOpenAI
from langchain.schema.messages import HumanMessage

class OktaAzureChatOpenAI(AzureChatOpenAI):
    def __init__(
        self,
        okta_token: str,
        openai_api_base: str,
        openai_api_version: str,
        deployment_name: str,
        temperature: float = 0.7,
    ):
        super().__init__(
            openai_api_base=openai_api_base,
            openai_api_version=openai_api_version,
            deployment_name=deployment_name,
            temperature=temperature,
            openai_api_key="dummy",  # Required by LangChain, ignored by your proxy
            openai_api_type="azure",
            openai_request_kwargs={
                "headers": {
                    "Authorization": f"Bearer {okta_token}"
                }
            }
        )

# ----- Usage Example -----

chat_model = OktaAzureChatOpenAI(
    okta_token="<your_okta_token>",                    # Replace with real Okta token
    openai_api_base="https://your-proxy-endpoint",     # Your proxy that talks to Azure OpenAI
    openai_api_version="2023-05-15",                   # Use the API version from Azure portal
    deployment_name="your-deployment-name",            # Your Azure OpenAI deployment name
    temperature=0.7
)

response = chat_model([
    HumanMessage(content="How do I secure OpenAI with Okta?")
])

print(response.content)
