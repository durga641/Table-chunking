use role sysadmin;

-- External Stage
CREATE or replace STAGE SYS_UTILS.SOURCE.TABLE_BACKUP_STAGE 
URL = 's3://datÃ¦ng-snowflake-prd-ascap/snowflake/' 
CREDENTIALS = (AWS_KEY_ID = '***************' AWS_SECRET_KEY = '*************');

-- FileFormat stage
CREATE OR REPLACE FILE FORMAT SYS_UTILS.SOURCE.TABLE_BACKUP_FF 
TYPE = 'CSV' 
COMPRESSION = 'AUTO' 
FIELD_DELIMITER = ',' 
RECORD_DELIMITER = '\n' 
SKIP_HEADER = 0 
FIELD_OPTIONALLY_ENCLOSED_BY = '\042' 
TRIM_SPACE = FALSE 
ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE 
ESCAPE = '\134' 
ESCAPE_UNENCLOSED_FIELD = '\134'
DATE_FORMAT = 'AUTO' 
TIMESTAMP_FORMAT = 'AUTO' 
NULL_IF = ('NULL');

-- Table DDL
create or replace TABLE SYS_UTILS.SOURCE.tablebkup_workload (
    DBNAME VARCHAR(100),
    SCHEMANAME VARCHAR(100),
    TABLENAME VARCHAR(100),
    FREQUENCY VARCHAR(100),
    LASTBKUPDT DATE,
    DEL_FL VARCHAR(1),
    FULL_INCR VARCHAR(10),
    INCR_WHERE_CLAUSE VARCHAR(100)
);


copy_script = f'''
snowsql -c serv_source -o exit_on_error=true -q "
USE DATABASE {db_nm}; 
USE SCHEMA {sch_nm}; 

COPY INTO @SYS_UTILS_SOURCE.TABLE_BACKUP_STAGE/{csv_file}.gz
FROM (SELECT * FROM {table_name})
FILE_FORMAT = (FORMAT_NAME = 'SYS_UTILS_SOURCE.TABLE_BACKUP_FF')
OVERWRITE = TRUE
SINGLE = TRUE;  -- Ensures a single file
"
'''


import boto3
from urllib.parse import urlparse
import re

def clean_old_files(base_path, file_prefix, num_backups):
    """
    Searches for files with the given prefix in the specified S3 path.
    Retains only the latest 'num_backups' backup sets, deleting older ones.

    Args:
    - base_path (str): The full S3 URL (e.g., s3://bucket-name/folder/subfolder/).
    - file_prefix (str): The prefix of the files to search for (e.g., "tab_name_").
    - num_backups (int): The number of latest backups to retain.
    """
    s3 = boto3.client('s3')

    # Parse the S3 URL
    parsed_url = urlparse(base_path)
    bucket_name = parsed_url.netloc  # Extracts the bucket name
    base_prefix = parsed_url.path.lstrip('/')  # Extracts the prefix without leading "/"

    # List objects in the specified prefix
    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=base_prefix)

    if 'Contents' not in response:
        print(f"No files found in: s3://{bucket_name}/{base_prefix}")
        return

    # Filter files that match the given prefix
    matching_files = [obj['Key'] for obj in response['Contents'] if obj['Key'].startswith(f"{base_prefix}/{file_prefix}")]

    if not matching_files:
        print(f"No files found with prefix '{file_prefix}' in: s3://{bucket_name}/{base_prefix}")
        return

    # Extract unique timestamps from file names (ignoring partition suffix)
    timestamp_pattern = re.compile(rf"{file_prefix}(\d{{4}}-\d{{2}}-\d{{2}}-\d{{2}}-\d{{2}}-\d{{2}})_")
    backup_groups = {}

    for file in matching_files:
        match = timestamp_pattern.search(file)
        if match:
            timestamp = match.group(1)
            if timestamp not in backup_groups:
                backup_groups[timestamp] = []
            backup_groups[timestamp].append(file)

    # Sort timestamps in descending order (latest first)
    sorted_timestamps = sorted(backup_groups.keys(), reverse=True)

    # Identify old backups to delete (if more than num_backups exist)
    if len(sorted_timestamps) > num_backups:
        old_timestamps = sorted_timestamps[num_backups:]
        files_to_delete = [file for ts in old_timestamps for file in backup_groups[ts]]

        for file in files_to_delete:
            s3.delete_object(Bucket=bucket_name, Key=file)
            print(f"Deleted: s3://{bucket_name}/{file}")
    else:
        print(f"No action required. {len(sorted_timestamps)} backups found, keeping {num_backups}.")

# Example Usage
base_path = "s3://your-s3-bucket-name/folder/subfolder"  # Base path including directories
file_prefix = "tab1_"  # Prefix of backup files
num_backups = 4  # Number of backups to retain

clean_old_files(base_path, file_prefix, num_backups)
