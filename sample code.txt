use role sysadmin;

-- External Stage
CREATE or replace STAGE SYS_UTILS.SOURCE.TABLE_BACKUP_STAGE 
URL = 's3://datÃ¦ng-snowflake-prd-ascap/snowflake/' 
CREDENTIALS = (AWS_KEY_ID = '***************' AWS_SECRET_KEY = '*************');

-- FileFormat stage
CREATE OR REPLACE FILE FORMAT SYS_UTILS.SOURCE.TABLE_BACKUP_FF 
TYPE = 'CSV' 
COMPRESSION = 'AUTO' 
FIELD_DELIMITER = ',' 
RECORD_DELIMITER = '\n' 
SKIP_HEADER = 0 
FIELD_OPTIONALLY_ENCLOSED_BY = '\042' 
TRIM_SPACE = FALSE 
ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE 
ESCAPE = '\134' 
ESCAPE_UNENCLOSED_FIELD = '\134'
DATE_FORMAT = 'AUTO' 
TIMESTAMP_FORMAT = 'AUTO' 
NULL_IF = ('NULL');

-- Table DDL
create or replace TABLE SYS_UTILS.SOURCE.tablebkup_workload (
    DBNAME VARCHAR(100),
    SCHEMANAME VARCHAR(100),
    TABLENAME VARCHAR(100),
    FREQUENCY VARCHAR(100),
    LASTBKUPDT DATE,
    DEL_FL VARCHAR(1),
    FULL_INCR VARCHAR(10),
    INCR_WHERE_CLAUSE VARCHAR(100)
);


copy_script = f'''
snowsql -c serv_source -o exit_on_error=true -q "
USE DATABASE {db_nm}; 
USE SCHEMA {sch_nm}; 

COPY INTO @SYS_UTILS_SOURCE.TABLE_BACKUP_STAGE/{csv_file}.gz
FROM (SELECT * FROM {table_name})
FILE_FORMAT = (FORMAT_NAME = 'SYS_UTILS_SOURCE.TABLE_BACKUP_FF')
OVERWRITE = TRUE
SINGLE = TRUE;  -- Ensures a single file
"
'''


import boto3
from urllib.parse import urlparse

def clean_old_files(base_path, file_prefix):
    """
    Searches for files with the given prefix in the specified S3 path.
    Retains only the latest 4 files, deleting older ones.

    Args:
    - base_path (str): The full S3 URL (e.g., s3://bucket-name/folder/subfolder/).
    - file_prefix (str): The prefix of the files to search for (e.g., "backup_").
    """
    s3 = boto3.client('s3')

    # Parse the S3 URL
    parsed_url = urlparse(base_path)
    bucket_name = parsed_url.netloc  # Extracts the bucket name
    base_prefix = parsed_url.path.lstrip('/')  # Extracts the prefix without leading "/"

    # List objects in the specified prefix
    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=base_prefix)

    if 'Contents' not in response:
        print(f"No files found in: s3://{bucket_name}/{base_prefix}")
        return

    # Filter files that match the given prefix
    matching_files = [obj for obj in response['Contents'] if obj['Key'].startswith(f"{base_prefix}/{file_prefix}")]

    if not matching_files:
        print(f"No files found with prefix '{file_prefix}' in: s3://{bucket_name}/{base_prefix}")
        return

    # Sort files by LastModified (newest first)
    matching_files.sort(key=lambda obj: obj['LastModified'], reverse=True)

    # If more than 4 files, delete older ones
    if len(matching_files) > 4:
        files_to_delete = matching_files[4:]  # Keep the latest 4, delete the rest

        for file in files_to_delete:
            s3.delete_object(Bucket=bucket_name, Key=file['Key'])
            print(f"Deleted: s3://{bucket_name}/{file['Key']}")
    else:
        print("No action required. Files are 4 or less.")

# Example Usage
base_path = "s3://your-s3-bucket-name/folder/subfolder"  # Base path including directories
file_prefix = "backup_"  # Only consider files starting with "backup_"

clean_old_files(base_path, file_prefix)
