use role sysadmin;

-- External Stage
CREATE or replace STAGE SYS_UTILS.SOURCE.TABLE_BACKUP_STAGE 
URL = 's3://datÃ¦ng-snowflake-prd-ascap/snowflake/' 
CREDENTIALS = (AWS_KEY_ID = '***************' AWS_SECRET_KEY = '*************');

-- FileFormat stage
CREATE OR REPLACE FILE FORMAT SYS_UTILS.SOURCE.TABLE_BACKUP_FF 
TYPE = 'CSV' 
COMPRESSION = 'AUTO' 
FIELD_DELIMITER = ',' 
RECORD_DELIMITER = '\n' 
SKIP_HEADER = 0 
FIELD_OPTIONALLY_ENCLOSED_BY = '\042' 
TRIM_SPACE = FALSE 
ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE 
ESCAPE = '\134' 
ESCAPE_UNENCLOSED_FIELD = '\134'
DATE_FORMAT = 'AUTO' 
TIMESTAMP_FORMAT = 'AUTO' 
NULL_IF = ('NULL');

-- Table DDL
create or replace TABLE SYS_UTILS.SOURCE.tablebkup_workload (
    DBNAME VARCHAR(100),
    SCHEMANAME VARCHAR(100),
    TABLENAME VARCHAR(100),
    FREQUENCY VARCHAR(100),
    LASTBKUPDT DATE,
    DEL_FL VARCHAR(1),
    FULL_INCR VARCHAR(10),
    INCR_WHERE_CLAUSE VARCHAR(100)
);


copy_script = f'''
snowsql -c serv_source -o exit_on_error=true -q "
USE DATABASE {db_nm}; 
USE SCHEMA {sch_nm}; 

COPY INTO @SYS_UTILS_SOURCE.TABLE_BACKUP_STAGE/{csv_file}.gz
FROM (SELECT * FROM {table_name})
FILE_FORMAT = (FORMAT_NAME = 'SYS_UTILS_SOURCE.TABLE_BACKUP_FF')
OVERWRITE = TRUE
SINGLE = TRUE;  -- Ensures a single file
"
'''



import boto3

def clean_old_files(base_path, db_nm, sch_nm, tab_nm):
    """
    Deletes older files in an S3 bucket path, keeping only the latest 4.
    
    Args:
    - base_path (str): The S3 bucket name or base path.
    - db_nm (str): Database name.
    - sch_nm (str): Schema name.
    - tab_nm (str): Table name.
    """
    s3 = boto3.client('s3')

    # Extract bucket name and prefix (folder path in S3)
    bucket_name = base_path.split('/')[2]  # Extract bucket name from s3://bucket-name
    prefix = f"{db_nm}/{sch_nm}/{tab_nm}/"

    # List objects in the specified prefix
    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)

    if 'Contents' not in response:
        print("No files found in the specified S3 path.")
        return

    # Sort files by LastModified (newest first)
    files = sorted(response['Contents'], key=lambda obj: obj['LastModified'], reverse=True)

    # If more than 5 files, keep the latest 4 and delete the rest
    if len(files) > 5:
        files_to_delete = files[4:]  # Keep the latest 4, delete the rest

        for file in files_to_delete:
            s3.delete_object(Bucket=bucket_name, Key=file['Key'])
            print(f"Deleted: s3://{bucket_name}/{file['Key']}")
    else:
        print("No action required. Files are 4 or less.")

# Example Usage
base_path = "s3://your-s3-bucket-name"
db_nm = "database"
sch_nm = "schema"
tab_nm = "table"

clean_old_files(base_path, db_nm, sch_nm, tab_nm)
